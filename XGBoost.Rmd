---
title: "Tutorial: Using XGBoost for Species-Distribution-Modelling"
output: 
  html_document:
    theme: united
    numbered: TRUE
    number_section: TRUE
    toc: TRUE
    toc_float: TRUE
---

<style type="text/css">

 body .main-container {
        max-width: 2000px;
    }
    
{ /* Normal  */
      font-size: 12px;
  }
td {  /* Table  */
  font-size: 9px;
}
h1.title {
  font-size: 20px;
  color: DarkRed;
}
h1 { /* Header 1 */
  font-size: 18px;
  color: DarkRed;
}
h2 { /* Header 2 */
    font-size: 16px;
  color: DarkRed;
}
h3 { /* Header 3 */
  font-size: 14px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 12px;
}
</style>

<!-- style for image slider  -->
<style>
  * {box-sizing:border-box}

/* Slideshow container */
.slideshow-container {
  max-width: 1000px;
  position: relative;
  margin: auto;
}

/* Hide the images by default */
.mySlides {
  display: none;
  padding: 0px 30px;
}

/* Next & previous buttons */
.prev, .next {
  cursor: pointer;
  position: absolute;
  top: 50%;
  width: auto;
  margin-top: -22px;
  padding: 16px;
  color: white;
  font-weight: bold;
  font-size: 18px;
  transition: 0.6s ease;
  border-radius: 0 3px 3px 0;
  user-select: none;
  background-color: rgba(0,0,0,0.8);
}

/* Position the "next button" to the right */
.next {
  right: 0;
  border-radius: 3px 0 0 3px; 
}

/* On hover, add a black background color with a little bit see-through */
.prev:hover, .next:hover {
  background-color: rgba(0,0,0,0.8);
}

/* Caption text */
.text_old {
  color: #f2f2f2;
  font-size: 15px;
  padding: 8px 12px;
  position: absolute;
  bottom: 8px;
  width: 100%;
  text-align: center;
}

.text {
  color: black;
  font-size: 15px;
  padding: 8px 12px;
  width: 100%;
  text-align: center;
  min-height: 60px;
}

/* The dots/bullets/indicators */
.dot {
  cursor: pointer;
  height: 15px;
  width: 15px;
  margin: 0 2px;
  background-color: #bbb;
  border-radius: 50%;
  display: inline-block;
  transition: background-color 0.6s ease;
}

.active, .dot:hover {
  background-color: #717171;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# ) Dummy notes area

https://juliasilge.com/blog/baseball-racing/
XGBoost-Docs: https://xgboost.readthedocs.io/en/stable/
https://dieghernan.github.io/tidyterra/reference/geom_spatraster.html
https://www.youtube.com/watch?v=gKyUucJwD8U&list=WL&index=71
https://www.geeksforgeeks.org/xgboost/

cbind: besser merge benutzen

```{r, include=FALSE}
#IMAGESLIDESHOW########################
```

# ) Introduction

The following Tutorial is the final assessment of the project seminar “Species Distribution Modeling” at Philipps-University Marburg. In this tutorial  we're going to use the XGBoost algorithm to predict the specie´s distribution of butterflies in Pakistan and create a species richness map of the country. [XGBoost](https://cran.r-project.org/web/packages/xgboost/xgboost.pdf) (eXtreme Gradient Boosting) is a popular machine learning algorithm that belongs to the family of gradient boosting methods. It was developed by [Tianqi Chen](https://tqchen.com/). and uses a combination of gradient boosting, decision trees, regularization, gradient-based optimization, feature importance analysis, parallelization. All this make's it a robust and powerful algorithm that often delivers state-of-the-art results in various machine learning tasks.
You will be introduced to the basic concepts of XGBoost and you'll be provided with a reproducible workflow to use XGBoost to build classification models.


# ) But what's XGBoost?

XGBoost is a ensemble Method same as Random Forrest, this means it combines the output of multiple Trees .But the methods differ in the  way the idividual TRees are build and how the results are combined.
In Xgb the Output oft he Trees anrent combined equally. Instead xgb uses a method called boosting.
Boosting combines Weak learner (WEAK LERNERS ERKLÄREN) sequentually so that the new tree corrects the errors oft he previous one.
To understand this we have to look into some mathematical details. But dont worry when using XGBoost these details will be automated. Nevertheless its importand to understand these details to optimize the algorithm later on.


## ) How does it exactly work?

As said XGBoost builds on many concepts to deliver (SOLCHE KRASSEN ERGEBNISSE!?)
We're going to start with the mathematical base concepts of how XGBoost builds trees.


<!-- Slideshow container -->
<div class="slideshow-container">

<div class="mySlides">
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhrAdT9U5a6js_clc19U2MySmViR5NAdMz5acOyTkolprWMt5-xYqLO8LgRgnCtHyt3UdKScfZtVNuoSkWNu8DfJe8aOeH4cxn1uWd7AarUOSMMOT5IPmMYegkyuxvN6qSBp26iFepdLY4FvCGs5XW9aO8rjT7tpsVNaotlPT0TcCWalyMXhddxZHwLxng/s1600/image1.jpeg" style="width:100%">  
<div class="text">Green dots represent the presence points. Red dots represent absence points. The black line in the middle is XGBoost's initial prediction. By default, it is 0.5, which means there is a 50% chance to find a butterfly at any given point.</div>
</div>

<div class="mySlides">
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDNS7NaU14obTeIdPoNMZGf3zl6y5PtL6RFqW4R9DpyQnJ08VoXx656ubQ4YYmn7SkdDI8TSyfnZtZeLmZ0rdd1gdsOn1vnVsmow7wsfptuMCMm7cxQyxHs6JVcdqhE7pMaJ6e6mP7abHtEkrnOelaK0oIBGpnWhpoEnldhUgK3EFyBqT_8JAyAYIDHg4/s1600/image2.jpeg" style="width:100%">
<div class="text">If the observed value is 1 (i.e., a presence point), the residuals are 0.5. The same applies to values of 0 (i.e., absence points) where the residuals are -0.5.</div>
</div>

<div class="mySlides">
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRsgLPZq5EfK0nt-jMyi2dRi3dg0CXvEGYVaKdTCZuKMYVymUEGzV3dTlGZ8biZnw2nZuU64y6bQnEz2MzByxq_oc_KMakRqAqpSuAY8yNvqLS-eGfpLXLRGsFzeDMpGH20jEoejFqcOoL56z3uWfHI-h2-tLvnEPZgCxdjn8VQfCJGsCH7lVY3W1qWWg/s1600/image3.jpeg" style="width:100%">
<div class="text">XGBoost now splits the observations at thresholds that result in the highest gain value.</div>
</div>
<div class="mySlides">
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhM9-yn-gtedu2-iH4cr4yfeC4zVSV-vaXaPCNIL2ZTi1-UdAhdBtopQDIlPkeWj85zUpy5yVD9vMMn-M3wyTMUPcDplwORV6qGLA4SkLhPGsUlzy-YOOvyBUV40M330ReHUC-hVXpn5f0aZeCc2Fyeogfe51Ci8DIMI2qxM1LTqnsbVTWn2pFTIx0nqM/s1600/image4.jpeg" style="width:100%">
<div class="text">XGBoost now splits the observations at thresholds that result in the highest gain value.</div>
</div>

  <!-- Next and previous buttons -->
  <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
  <a class="next" onclick="plusSlides(1)">&#10095;</a>
</div>

<!-- The dots/circles -->
<div style="text-align:center">
  <span class="dot" onclick="currentSlide(1)"></span>
  <span class="dot" onclick="currentSlide(2)"></span>
  <span class="dot" onclick="currentSlide(3)"></span>
  <span class="dot" onclick="currentSlide(4)"></span>
</div>

<script>
let slideIndex = 1;
showSlides(slideIndex);

// Next/previous controls
function plusSlides(n) {
  showSlides(slideIndex += n);
}

// Thumbnail image controls
function currentSlide(n) {
  showSlides(slideIndex = n);
}

function showSlides(n) {
  let i;
  let slides = document.getElementsByClassName("mySlides");
  console.log(slides)
  let dots = document.getElementsByClassName("dot");
  console.log(dots)
  if (n > slides.length) {slideIndex = 1}
  if (n < 1) {slideIndex = slides.length}
  for (i = 0; i < slides.length; i++) {
    slides[i].style.display = "none";
  }
  for (i = 0; i < dots.length; i++) {
    dots[i].className = dots[i].className.replace(" active", "");
  }
  slides[slideIndex-1].style.display = "block";
  dots[slideIndex-1].className += " active";
}
</script>

# ) Species Distribution Modeling




# ) Extreme Gradient Boosting Algorithem




# ) Application of XGBoost in R

## ) Prerequisites

Before you dive into the code you need to install some packages this script will use: 

```{r Install packages, eval=FALSE}
install.packages('tidyverse')
install.packages("ggplot2")
install.packages('tidyterra')
install.packages('fastDummies')
install.packages('tidymodels')

#if(!require(dplyr)){install.packages('dplyr')}
```

The XGBoost package can by installed in two different ways. First there is the default package from CRAN, which will do it in most situations.
```{r Setup from CRAN, eval=FALSE, include=TRUE}
install.packages("xgboost")
```
But if you are dealing with large data sets you may want to use GPU acceleration. Therefor you have to use a prebuild package from GITHUB (https://github.com/dmlc/xgboost/releases). Download it, place it in the same folder as this script und run the commands below.
```{r Setup for GPU acceleration, eval=FALSE, include=TRUE}
# Install dependencies
system('R -q -e "install.packages(c(\'data.table\', \'jsonlite\'))"') # TODO: convert this to r code
# Install XGBoost
system(paste("R CMD INSTALL ", getwd(),  "/xgboost_r_gpu_win64_21d95f3d8f23873a76f8afaad0fee5fa3e00eafe.tar.gz", sep=""))
```

After installing all needed libaries you need to load them: 

```{r Loading libaries, eval=T, include=F, results='hide'}

# results = "hide"
# TODO: clean up this code
# TODO: disable output? but run it while knitting

require(dplyr)      # easy dataframe modification
require(geodata)    # downloading geospatial world dataset made easy


require(tidyverse)
require(tidymodels)
require(fastDummies)



require(xgboost)
require(terra) #TODO_ check if terry needed?
require(geodata)
require(dplyr)
require(tidyverse)
require(ggplot2)
require(tidyterra)
require(fastDummies)
require(tidymodels)
require(vip)
```

## ) Preparing data

Let's start with preparing the data used to train the model. The steps of the following script can be summarized in:

1. Load species occurrence data from csv file
2. Convert species to geospatial 'simple feature' object
3. Load border, bioclim and elevation for pakistan from geodata package
4. Generate random points inside pakistan as background points and extend them with a column for species = NA
5. Combine presence and absence (background) points into a single object
6. Extract bioclim and elevation values for the modeling_data 

```{r Preparing data}
##################################################
##########    Step 1 - Loading data     ##########
##################################################

# species oberservation data from pakistan
species_occurrences_all = read.table("data/PakistanLadakh.csv", sep=",", header=TRUE)
species_occurrences_all = sf::st_as_sf(species_occurrences_all, coords=c("x", "y"), remove=TRUE, crs=sf::st_crs("epsg:4326"))

# TODO: check level of border_pak data, is this the top level border?
# political border of pakistan
border_pak <- geodata::gadm(country='PAK', level=0, path='./data')

# bioclim data from pakistan
bioclim_pak = geodata::worldclim_country(country="PAK", res=10, var="bio", path="data/", version = "2.1")
names(bioclim_pak)<-substr(names(bioclim_pak), 11, 20) # TODO: rename to mare meaningfull names or show table of layers in text
bioclim_pak = terra::mask(bioclim_pak, border_pak)

# elevation data form pakistan
elevation_pak = geodata::elevation_30s(country='PAK', path='data/')


##################################################
##########  Step 2 - Data aggregation   ##########
##################################################

# presence points
species_presence = species_occurrences_all


# Generate random points inside pakistan as background points and extend them with a column for species = NA
# TODO: why 1000 points?? give a explanation for the decisison
border_pak = sf::st_as_sf(border_pak)
background_points = sf::st_sample(border_pak, size = 1000)

# TODO: add col for occurrence = 1 or occurrence = 0 here, dunmycols can then be trown away
# add species = NA to the backgroundpoints, needed for rbind to join the data
species_absence = cbind(background_points, data.frame(species = as.character(NA)))
species_absence = sf::st_as_sf(species_absence)


# Combine presence and absence (background) points into a single object
modeling_data_ = rbind(species_presence, species_absence)
# Only points inside Pakistan should be used for modeling, also remove the columns added by the intersection. 
modeling_data_ = sf::st_intersection(modeling_data_, border_pak) %>% select(-COUNTRY, -GID_0)


# Extract values from bioclim and elevation, join them to our modeling_data
extraction_bioclim_pak = terra::extract(bioclim_pak, modeling_data_, bind=FALSE, ID=FALSE)
extraction_elevation_pak = terra::extract(elevation_pak, modeling_data_, bind=FALSE, ID=FALSE)
modeling_data_extracted = cbind( modeling_data_, extraction_bioclim_pak, extraction_elevation_pak)


# create a final data variable and clean up variables
modeling_data = modeling_data_extracted

rm(species_occurrences_all); rm(species_presence); rm(background_points); rm(species_absence); rm(modeling_data_); rm(extraction_bioclim_pak); rm(extraction_elevation_pak); rm(modeling_data_extracted)
```

## ) Data overview

Check your results:

Are there presence and absence points inside the modeling_data ? Check if the species column has names (=presence) and NA (=absence) values.
```{r Data overview 1}
head(modeling_data); tail(modeling_data)
```

Is the elevation data correctly loaded? Plot a map:

```{r Data overview 2}
ggplot() + 
  geom_spatraster(data = elevation_pak) +
  geom_sf(data = sf::st_as_sf(border_pak), fill=NA, show.legend=FALSE) +
  scale_fill_hypso_c(name="Elevation")
```

Is the bioclim data correctly loaded? Plot a map:

```{r  Data overview 3, fig.width=15}
# TODO: set value scale to be individual for every plot.
ggplot() + 
  geom_spatraster(data = bioclim_pak) +
  facet_wrap(~lyr) +
  geom_sf(data = sf::st_as_sf(border_pak), fill=NA, show.legend=FALSE) + 
  ggtitle("Bioclim data of Pakistan")


#for (layer in names(bioclim_pak)) {
#  ggplot() + 
#    geom_spatraster(data = bioclim_pak[layer]) +
#    geom_sf(data = sf::st_as_sf(border_pak), fill=NA, show.legend=FALSE) + 
#    ggtitle(paste("Bioclim", layer, "data of Pakistan")) 
#}
```


How is the modeling data spatially distributed? Plot a map:

```{r  Data overview 4}
ggplot() + 
  geom_sf(data = sf::st_as_sf(border_pak), mapping=aes(alpha = 0), show.legend = FALSE) + 
  geom_sf(data = sf::st_as_sf(modeling_data), mapping=aes(color=species), show.legend = FALSE) +
  ggtitle("Oberserved occurrence of butterflies in Pakistan plus background points")
```

** How many samples do the species data have? **

```{r Data overview 5}
species_nsamples = data.frame(modeling_data) %>% 
                    count(species, sort=TRUE) %>% 
                    rename(n_samples = n) %>% 
                    filter(!is.na(species))

ggplot(species_nsamples, aes(n_samples)) +
       geom_histogram(binwidth = 5) +
       geom_vline(aes(xintercept=mean(n_samples)), linetype="dashed") +
       annotate(x=mean(species_nsamples$n_samples), y=+Inf, label=paste("Mean:",round(mean(species_nsamples$n_samples),2)), vjust=3, geom="label") +
       labs(x = "Number of Samples", y = "Number of Species")

```


## ) Training the model

```{r Traning the model}
#for(sp in (modeling_data$species[16]))
for(sp in (modeling_data %>% distinct(species) %>% filter(!is.na(species)))$species)
{
  message(paste("Training model for species", sp))
  
  # filter modeling data to current species, don't forget the absence points!
  data = modeling_data %>% filter(species == sp | is.na(species))
  data <- dummy_columns(data, select_columns = 'species',
                        ignore_na = TRUE, 
                        remove_selected_columns = TRUE)
  
  # make dummy_columns name persistent over all species
  # replace NA values with 0
  data <- data %>% 
            rename(species_occurrence = starts_with("species_")) %>%
            mutate(species_occurrence = ifelse(is.na(species_occurrence), 0, species_occurrence)) %>%
            mutate(species_occurrence = factor(species_occurrence))
  
  training_data <- xgb.DMatrix(data = as.matrix(data %>% select(-species_occurrence, -geometry)), 
                               label = as.matrix(data %>% select(species_occurrence))
                               )
  # check the parameters here https://xgboost.readthedocs.io/en/latest/paramet er.html
  xgb_model <- xgb.train(data = training_data,
                       verbosity = 3, # 0 (silent), 1 (warning), 2 (info), 3 (debug)
                       
                       eta = 0.3, # learning rate, default = 0.3
                       gamma = 0, # Minimum loss reduction, default = 0
                       max.depth = 6, # default = 6
                       #sampling_method = "gradient_based", # default = uniform
                       #lampda = 1,
                       alpha = 0,
                       #three_method = "gpu_hist",
                       
                       #nthread = 2, 
                       nrounds = 20,
                       objective = "binary:logistic"
                       #objective = "reg:squaredlogerror" # 
                       # save_name = paste("out/", sp, "/" ,sp, ".model", sep = "") # path to save model
                       )
  
  #xgb_model[["evaluation_log"]]
  
  importance <- xgb.importance(model = xgb_model)
  #xgb.plot.importance(importance_matrix = importance)
  
  #xgb.ggplot.deepness(xgb_model)
  
  xgb.ggplot.shap.summary(data = as.matrix(data %>% select(-species_occurrence, -geometry)), model = xgb_model )
  ggsave(paste(sp, "_shap.png", sep=""), path=paste("out/",sp, sep=""))

  # TODO: create folder beforhand
  xgb.save(xgb_model, paste("out/", sp, "/" ,sp, ".model", sep = ""))
  
  # Cleanup before next iteration
  rm(sp); rm(data); rm(training_data); rm(xgb_model); rm(importance)
}

```


```{r, eval=F, include=F}
 # tracking runtime
start.time <- Sys.time()
# current runtime for all species ~35min

# use this for development to shorten runtime
#final_modeling_data_save = final_modeling_data
#final_modeling_data = final_modeling_data_save
#final_modeling_data = final_modeling_data %>% filter(species %in% species_names[1:50] | is.na(species))

for( sp in (final_modeling_data %>% distinct(species) %>% filter(!is.na(species)))$species )
{
  print(paste("Sarting compute for species:", sp))
  tryCatch(
    {
      # TODO: select only used cols
      data = final_modeling_data %>% 
        filter(species == sp | is.na(species)) %>% 
        select(ends_with(sp), matches("elv"), starts_with("bio")) %>%
        rename_at( 1, ~"species_occurrence") %>%
        mutate(species_occurrence = ifelse(is.na(species_occurrence), 0, species_occurrence)) %>%
        mutate(species_occurrence = factor(species_occurrence))
      
      #str(data)
      #message(data)
      
      # using "final_modeling_data" because of missing geometry in "data"
      ggplot() + 
        geom_sf(data = sf::st_as_sf(border), mapping=aes(alpha = 0), show.legend = FALSE) + 
        geom_sf(data = sf::st_as_sf(final_modeling_data %>% filter(species == sp)), mapping=aes(color=species))
      # Save the plot for every species in out/<species>/<species>.png
      ggsave(paste(sp, ".png", sep=""), path=paste("out/",sp, sep=""))
      
      ########## Training the model ##########
      xgboost_recipe <- 
        recipe( species_occurrence ~ . , data = data)
      
      xgboost_model <- 
        boost_tree(
        trees = 500,
        min_n = 2,
        mtry = 3,
        learn_rate = 0.01
      ) %>%
        set_engine("xgboost") %>%
        set_mode("classification")

      xgboost_workflow <- workflow() %>% add_recipe(xgboost_recipe) %>% add_model(xgboost_model)

      xgboost_fit <- xgboost_workflow %>% 
        fit(data = data)
      
      xgboost_fit %>% 
        extract_fit_parsnip() %>%
        vip(geom = "point", num_features = 15)
      ggsave(paste(sp, "_fit.png", sep=""), path=paste("out/",sp, sep=""))
      
      
      # TODO: create new random point dataset of pakistan with environmental data and high density 
      # for a better spatial resultion of the prediction
      predicted = augment(xgboost_fit, final_modeling_data)
      
      predicted = sf::st_as_sf( predicted %>% select(!starts_with("species_")) )

      ggplot() + 
        geom_spatraster(data = elevation) + 
        scale_fill_hypso_c() + 
        geom_sf(data = sf::st_as_sf(border), mapping=aes(alpha = 0), show.legend = FALSE) +
        #geom_sf(data = sf::st_as_sf(data), mapping = aes(color = 'bg')) + 
        geom_sf(data = predicted %>% filter(species == sp), mapping = aes(color = 'observed'), size = 5) +
        geom_sf(data = predicted %>% filter(.pred_class == 1), mapping = aes(color = 'predicted'), size = 3 )
      
      ggsave(paste(sp, "_model.png", sep=""), path=paste("out/",sp, sep=""))
      
    },
    error=function(cond)
    {
      message("Error:")
      message(cond)
    },
    warning=function(cond)
    {
      message("Warning:")
      message(cond)
    },
    finally={
      message("finally")
    }
  )
  print("finished")
}

# tracking runtime
end.time <- Sys.time()
diff <- end.time - start.time
diff

```

```{r, eval=F, include=F, fig.width=15}

# only for development, filter to one species ## DO not filter here since absence points will be filtered out
modeling_data_save = modeling_data
modeling_data = modeling_data %>% select(c(-geometry, -species))
modeling_data
str(modeling_data)
# resulst in getting a dataset with 52 samples pf aglais caschirensis

predic_colname <- paste('species_', species_names[6], sep="")

# taken from https://www.tidymodels.org/start/recipes/
xgboost_recipe <- 
  recipe( species_Aglais_caschmirensis ~ . , data = modeling_data)

#summary(sdm_recipe)

#prep(sdm_recipe)

xgboost_model <- 
  boost_tree(
    trees = 1000,
    min_n = 2,
    mtry = 3,
    learn_rate = 0.01
  ) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

xgboost_workflow <- workflow() %>% add_recipe(xgboost_recipe) %>% add_model(xgboost_model)

xgboost_fit <- xgboost_workflow %>% 
  fit(data = modeling_data)


library(vip)
xgboost_fit %>% 
  extract_fit_parsnip() %>%
  vip(geom = "point", num_features = 15)


out = augment(xgboost_fit, modeling_data_save )

out

out = sf::st_as_sf(out)

ggplot() + 
  geom_spatraster(data = elevation) + 
  scale_fill_hypso_c() + 
  geom_sf(data = sf::st_as_sf(border), mapping=aes(alpha = 0), show.legend = FALSE) +
  geom_sf(data = sf::st_as_sf(modeling_data_save), mapping = aes(color = 'bg')) + 
  geom_sf(data = out %>% filter(species_Aglais_caschmirensis == 1), mapping = aes(color = 'observed'), size = 5) +
  geom_sf(data = out %>% filter(.pred_class == 1), mapping = aes(color = 'predicted'), size = 3 )



```

```{r, eval=F, include=F}
###             ###
### not working ###
###             ###  

modeling_data

dtrain <- 
  xgb.DMatrix(
    data = as.matrix(modeling_data %>% select(-species_Aglais_caschmirensis)), 
    label = as.matrix(modeling_data %>% select(species_Aglais_caschmirensis))
    )


xgb_model <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 200, objective = "binary:logistic", verbose = T)

xgb.plot.shap( data = as.matrix(modeling_data %>% select(-species_Aglais_caschmirensis)), model = xgb_model, top_n=7)

#xgboost_pred <- predict(model, as.matrix(species_absence))



```

## ) Prediction

Steps:
1. Generate Point Grid for pakistan, point_distance = xx meters
2. Extract environmental data for point grid
3. Predict occurrence based on the build models
3. Rasterrize points

```{r}
# TODO: vectorize raster of pakistan
prediction_points_raw = sf::st_sample(border_pak, type="regular", size = 5000)

extraction_bioclim_pak   = terra::extract(bioclim_pak, vect(prediction_points_raw), bind=FALSE, ID=FALSE)
extraction_elevation_pak = terra::extract(elevation_pak, vect(prediction_points_raw), bind=FALSE, ID=FALSE)
prediction_points_extracted = cbind( prediction_points_raw, extraction_bioclim_pak, extraction_elevation_pak)

prediction_points = xgb.DMatrix(data = as.matrix(prediction_points_extracted %>% select(-geometry)))

# cleanup
rm(extraction_bioclim_pak); rm(extraction_elevation_pak);

#for(sp in (modeling_data$species[1]))
for(sp in (modeling_data %>% distinct(species) %>% filter(!is.na(species)))$species)
{
  sp_model = xgb.load(paste("out/", sp, "/" ,sp, ".model", sep = ""))
  prediction <- predict(sp_model, prediction_points)
  
  prediction_as_binary = cbind(prediction, prediction_points_extracted) %>% filter(prediction > 0.60)
  
  # TODO: change symbology to observed = x and predicted = o
  # TODO: add error rms to plot
  ggplot() + 
    geom_sf(data = sf::st_as_sf(border_pak), mapping=aes(alpha = 0), show.legend = FALSE) + 
    geom_sf(data = modeling_data %>% filter(species == sp), mapping = aes(color = 'observed'), size = 2) +
    geom_sf(data = sf::st_as_sf(prediction_as_binary, crs="epsg:4326"), mapping = aes(color = "predicted > 0.60"))
  
  ggsave(paste(sp, ".png", sep=""), path=paste("out/",sp, sep=""))
  
  rm(sp); rm(sp_model); rm(prediction); rm(prediction_points_raw); rm(prediction_as_binary)
}

rm(prediction_points)
```

```{r, eval=F, include=F}
for(sp in (modeling_data$species[1]))
#for(sp in (modeling_data %>% distinct(species) %>% filter(!is.na(species)))$species)
{

# merge bioclim_pak and elevation_pak into one raster as different layers:
collection = c(bioclim_pak, elev_pak)
collection
elev_pak = resample(elevation_pak, bioclim_pak)

ext(elevation_pak) <- ext(bioclim_pak)

ggplot() + 
  geom_spatraster(data = collection) +
  facet_wrap(~lyr) +
  geom_sf(data = sf::st_as_sf(border_pak), fill=NA, show.legend=FALSE) + 
  ggtitle("Bioclim data of Pakistan")

  prediction_data<- xgb.DMatrix(data = as.matrix(collection)) 


model = xgb.load(paste("out/", sp, "/" ,sp, ".model", sep = ""))
# use terra to predict directly from spatraster:
prediction <- predict(model, prediction_data)

prediction <- as.numeric(prediction > 0.5)



}

# TODO: show two different species with prediction variance

vect
# TODO: Rasterize prediction 
# Prediction > 0,5 -> 1, > 0,5 -> 0
```

Interessting species predictions:
- Tarucus_theophrastus
- Lethe_verma
- Junonia_iphita


predictions were made for points outside pakistan. There should be no data extracted for those points. How ist this possible?
Cut samples with polygon of pakistan!

Species with small number of samples resuting in no prediction. Where ist the min the predicton works? Check Species with 1, 2, 3, 4, 5 Samples and so on 






## ) Species Richness Map

```{r}

```


# ) Conclusion

How was Computing Speed compared to other methods? 


print sessioninfo