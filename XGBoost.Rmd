---
title: "Tutorial: Using XGBoost for Species-Distribution-Modelling"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Dummy notes area

https://juliasilge.com/blog/baseball-racing/
XGBoost-Docs: https://xgboost.readthedocs.io/en/stable/
https://dieghernan.github.io/tidyterra/reference/geom_spatraster.html
https://www.youtube.com/watch?v=gKyUucJwD8U&list=WL&index=71



# Species Distribution Modeling




# Extreme Gradient Boosting Algorithem




# Application of XGBoost




## Prerequisites

Before you dive into the code you need to install some packages this script will use: 

```{r Install packages, eval=FALSE}
install.packages('tidyverse')
install.packages("ggplot2")
install.packages('tidyterra')
install.packages('fastDummies')
install.packages('tidymodels')

#if(!require(dplyr)){install.packages('dplyr')}
```

The XGBoost package can by installed in two different ways. First there is the default package from CRAN, which will do it in most situations.
```{r Setup from CRAN, eval=FALSE, include=TRUE}
install.packages("xgboost")
```
But if you are dealing with large data sets you may want to use GPU acceleration. Therefor you have to use a prebuild package from GITHUB (https://github.com/dmlc/xgboost/releases). Download it, place it in the same folder as this script und run the commands below.
```{r Setup for GPU acceleration, eval=FALSE, include=TRUE}
# Install dependencies
system('R -q -e "install.packages(c(\'data.table\', \'jsonlite\'))"') # TODO: convert this to r code
# Install XGBoost
system(paste("R CMD INSTALL ", getwd(),  "/xgboost_r_gpu_win64_21d95f3d8f23873a76f8afaad0fee5fa3e00eafe.tar.gz", sep=""))
```

After installing all needed libaries you need to load them: 

```{r Loading libaries, eval=T, include=F}
# TODO: clean up this code
# TODO: disable output? but run it while knitting

require(dplyr)      # easy dataframe modification
require(geodata)    # downloading geospatial world dataset made easy


require(tidyverse)
require(tidymodels)
require(fastDummies)



require(xgboost)
require(terra) #TODO_ check if terry needed?
require(geodata)
require(dplyr)
require(tidyverse)
require(ggplot2)
require(tidyterra)
require(fastDummies)
require(tidymodels)
require(vip)
```

## Preparing data

Let's start with preparing the data used to train the model. The steps of the following script can be summarized in:

1. Load species occurrence data from csv file
2. Convert species to geospatial 'simple feature' object
3. Load border, bioclim and elevation for pakistan from geodata package
4. Generate random points inside pakistan as background points and extend them with a column for species = NA
5. Combine presence and absence (background) points into a single object
6. Extract bioclim and elevation values for the modeling_data 

```{r Preparing data}
##################################################
##########    Step 1 - Loading data     ##########
##################################################

# species oberservation data from pakistan
species_occurrences_all = read.table("data/PakistanLadakh.csv", sep=",", header=TRUE)
species_occurrences_all = sf::st_as_sf(species_occurrences_all, coords=c("x", "y"), remove=TRUE, crs=sf::st_crs("epsg:4326"))

# TODO: check level of border_pak data, is this the top level border?
# political border of pakistan
border_pak <- geodata::gadm(country='PAK', level=0, path='./data')

# bioclim data from pakistan
bioclim_pak = geodata::worldclim_country(country="PAK", res=10, var="bio", path="data/", version = "2.1")
names(bioclim_pak)<-substr(names(bioclim_pak), 11, 20) # TODO: rename to mare meaningfull names or show table of layers in text
bioclim_pak = terra::mask(bioclim_pak, border_pak)

# elevation data form pakistan
elevation_pak = geodata::elevation_30s(country='PAK', path='data/')


##################################################
##########  Step 2 - Data aggregation   ##########
##################################################

# presence points
species_presence = species_occurrences_all


# Generate random points inside pakistan as background points and extend them with a column for species = NA
# TODO: why 1000 points?? give a explanation for the decisison
border_pak = sf::st_as_sf(border_pak)
background_points = sf::st_sample(border_pak, size = 1000)
# add species = NA to the backgroundpoints, needed for rbind to join the data
species_absence = cbind(background_points, data.frame(species = as.character(NA)))
species_absence = sf::st_as_sf(species_absence)


# Combine presence and absence (background) points into a single object
modeling_data_ = rbind(species_presence, species_absence)
# Only points inside Pakistan should be used for modeling, also remove the columns added by the intersection. 
modeling_data_ = sf::st_intersection(modeling_data_, border_pak) %>% select(-COUNTRY, -GID_0)


# Extract values from bioclim and elevation, join them to our modeling_data
extraction_bioclim_pak = terra::extract(bioclim_pak, modeling_data_, bind=FALSE, ID=FALSE)
extraction_elevation_pak = terra::extract(elevation_pak, modeling_data_, bind=FALSE, ID=FALSE)
modeling_data_extracted = cbind( modeling_data_, extraction_bioclim_pak, extraction_elevation_pak)

modeling_data = modeling_data_extracted


# clean up
rm(species_occurrences_all); rm(species_presence); rm(background_points); rm(species_absence); rm(modeling_data_); rm(extraction_bioclim_pak); rm(extraction_elevation_pak); rm(modeling_data_extracted)
```

## Data overview

Check your results:

Are there presence and absence points inside the modeling_data ? Check if the species column has names (=presence) and NA (=absence) values.
```{r Data overview 1}
head(modeling_data); tail(modeling_data)
```

Is the elevation data correctly loaded? Plot a map:

```{r Data overview 2}
ggplot() + 
  geom_spatraster(data = elevation_pak) +
  geom_sf(data = sf::st_as_sf(border_pak), fill=NA, show.legend=FALSE) +
  scale_fill_hypso_c(name="Elevation")
```

Is the bioclim data correctly loaded? Plot a map:

```{r  Data overview 3, fig.width=15}
# TODO: set value scale to be individual for every plot.
ggplot() + 
  geom_spatraster(data = bioclim_pak) +
  facet_wrap(~lyr) +
  geom_sf(data = sf::st_as_sf(border_pak), fill=NA, show.legend=FALSE) + 
  ggtitle("Bioclim data of Pakistan")


#for (layer in names(bioclim_pak)) {
#  ggplot() + 
#    geom_spatraster(data = bioclim_pak[layer]) +
#    geom_sf(data = sf::st_as_sf(border_pak), fill=NA, show.legend=FALSE) + 
#    ggtitle(paste("Bioclim", layer, "data of Pakistan")) 
#}
```

How is the modeling data spatially distributed? Plot a map:

```{r  Data overview 4}
ggplot() + 
  geom_sf(data = sf::st_as_sf(border_pak), mapping=aes(alpha = 0), show.legend = FALSE) + 
  geom_sf(data = sf::st_as_sf(modeling_data), mapping=aes(color=species), show.legend = FALSE) +
  ggtitle("Oberserved occurrence of butterflies in Pakistan plus background points")
```
How many samples do the species data have?
Show Min, Max, Avg, maybe Histogramm

```{r Data overview 5}
species_nsamples = data.frame(modeling_data) %>% 
                    count(species, sort=TRUE) %>% 
                    rename(n_samples = n) %>% 
                    filter(!is.na(species))

ggplot(species_nsamples, aes(n_samples)) +
       geom_histogram(binwidth = 5) +
       geom_vline(aes(xintercept=mean(n_samples)), linetype="dashed") +
       annotate(x=mean(species_nsamples$n_samples), y=+Inf, label=paste("Mean:",round(mean(species_nsamples$n_samples),2)), vjust=3, geom="label") +
       labs(x = "Number of Samples", y = "Number of Species")

```



## Training the model

```{r Traning the model, fig.width=20}
for(sp in (modeling_data$species[1]))
#for(sp in (modeling_data %>% distinct(species) %>% filter(!is.na(species)))$species)
{
  message(paste("Training model for species", sp))
  
  # filter modeling data to current species, don't forget the absence points!
  data = modeling_data %>% filter(species == sp | is.na(species))
  data <- dummy_columns(data, select_columns = 'species',
                        ignore_na = TRUE, 
                        remove_selected_columns = TRUE)
  
  # make dummy_columns name persistent over all species
  # replace NA values with 0
  data <- data %>% 
            rename(species_occurrence = starts_with("species_")) %>%
            mutate(species_occurrence = ifelse(is.na(species_occurrence), 0, species_occurrence)) %>%
            mutate(species_occurrence = factor(species_occurrence))
  
  training_data <- xgb.DMatrix(data = as.matrix(data %>% select(-species_occurrence, -geometry)), 
                               label = as.matrix(data %>% select(species_occurrence))
                               )
  
  xgb_model <- xgboost(data = training_data, 
                       max.depth = 10,
                       eta = 1, 
                       nthread = 2, 
                       nrounds = 20,
                       objective = "binary:logistic")
  
  #xgb_model[["evaluation_log"]]
  
  #importance <- xgb.importance(model = xgb_model)
  #xgb.plot.importance(importance_matrix = importance)
  
  #xgb.ggplot.deepness(xgb_model)
  
  xgb.ggplot.shap.summary(data = as.matrix(data %>% select(-species_occurrence, -geometry)),
                          model = xgb_model
                          )

}

```





```{r, eval=F, include=F}
 # tracking runtime
start.time <- Sys.time()
# current runtime for all species ~35min

# use this for development to shorten runtime
#final_modeling_data_save = final_modeling_data
#final_modeling_data = final_modeling_data_save
#final_modeling_data = final_modeling_data %>% filter(species %in% species_names[1:50] | is.na(species))

for( sp in (final_modeling_data %>% distinct(species) %>% filter(!is.na(species)))$species )
{
  print(paste("Sarting compute for species:", sp))
  tryCatch(
    {
      # TODO: select only used cols
      data = final_modeling_data %>% 
        filter(species == sp | is.na(species)) %>% 
        select(ends_with(sp), matches("elv"), starts_with("bio")) %>%
        rename_at( 1, ~"species_occurrence") %>%
        mutate(species_occurrence = ifelse(is.na(species_occurrence), 0, species_occurrence)) %>%
        mutate(species_occurrence = factor(species_occurrence))
      
      #str(data)
      #message(data)
      
      # using "final_modeling_data" because of missing geometry in "data"
      ggplot() + 
        geom_sf(data = sf::st_as_sf(border), mapping=aes(alpha = 0), show.legend = FALSE) + 
        geom_sf(data = sf::st_as_sf(final_modeling_data %>% filter(species == sp)), mapping=aes(color=species))
      # Save the plot for every species in out/<species>/<species>.png
      ggsave(paste(sp, ".png", sep=""), path=paste("out/",sp, sep=""))
      
      ########## Training the model ##########
      xgboost_recipe <- 
        recipe( species_occurrence ~ . , data = data)
      
      xgboost_model <- 
        boost_tree(
        trees = 500,
        min_n = 2,
        mtry = 3,
        learn_rate = 0.01
      ) %>%
        set_engine("xgboost") %>%
        set_mode("classification")

      xgboost_workflow <- workflow() %>% add_recipe(xgboost_recipe) %>% add_model(xgboost_model)

      xgboost_fit <- xgboost_workflow %>% 
        fit(data = data)
      
      xgboost_fit %>% 
        extract_fit_parsnip() %>%
        vip(geom = "point", num_features = 15)
      ggsave(paste(sp, "_fit.png", sep=""), path=paste("out/",sp, sep=""))
      
      
      # TODO: create new random point dataset of pakistan with environmental data and high density 
      # for a better spatial resultion of the prediction
      predicted = augment(xgboost_fit, final_modeling_data)
      
      predicted = sf::st_as_sf( predicted %>% select(!starts_with("species_")) )

      ggplot() + 
        geom_spatraster(data = elevation) + 
        scale_fill_hypso_c() + 
        geom_sf(data = sf::st_as_sf(border), mapping=aes(alpha = 0), show.legend = FALSE) +
        #geom_sf(data = sf::st_as_sf(data), mapping = aes(color = 'bg')) + 
        geom_sf(data = predicted %>% filter(species == sp), mapping = aes(color = 'observed'), size = 5) +
        geom_sf(data = predicted %>% filter(.pred_class == 1), mapping = aes(color = 'predicted'), size = 3 )
      
      ggsave(paste(sp, "_model.png", sep=""), path=paste("out/",sp, sep=""))
      
    },
    error=function(cond)
    {
      message("Error:")
      message(cond)
    },
    warning=function(cond)
    {
      message("Warning:")
      message(cond)
    },
    finally={
      message("finally")
    }
  )
  print("finished")
}

# tracking runtime
end.time <- Sys.time()
diff <- end.time - start.time
diff

```

```{r, eval=F, include=F, fig.width=15}

# only for development, filter to one species ## DO not filter here since absence points will be filtered out
modeling_data_save = modeling_data
modeling_data = modeling_data %>% select(c(-geometry, -species))
modeling_data
str(modeling_data)
# resulst in getting a dataset with 52 samples pf aglais caschirensis

predic_colname <- paste('species_', species_names[6], sep="")

# taken from https://www.tidymodels.org/start/recipes/
xgboost_recipe <- 
  recipe( species_Aglais_caschmirensis ~ . , data = modeling_data)

#summary(sdm_recipe)

#prep(sdm_recipe)

xgboost_model <- 
  boost_tree(
    trees = 1000,
    min_n = 2,
    mtry = 3,
    learn_rate = 0.01
  ) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

xgboost_workflow <- workflow() %>% add_recipe(xgboost_recipe) %>% add_model(xgboost_model)

xgboost_fit <- xgboost_workflow %>% 
  fit(data = modeling_data)


library(vip)
xgboost_fit %>% 
  extract_fit_parsnip() %>%
  vip(geom = "point", num_features = 15)


out = augment(xgboost_fit, modeling_data_save )

out

out = sf::st_as_sf(out)

ggplot() + 
  geom_spatraster(data = elevation) + 
  scale_fill_hypso_c() + 
  geom_sf(data = sf::st_as_sf(border), mapping=aes(alpha = 0), show.legend = FALSE) +
  geom_sf(data = sf::st_as_sf(modeling_data_save), mapping = aes(color = 'bg')) + 
  geom_sf(data = out %>% filter(species_Aglais_caschmirensis == 1), mapping = aes(color = 'observed'), size = 5) +
  geom_sf(data = out %>% filter(.pred_class == 1), mapping = aes(color = 'predicted'), size = 3 )



```

```{r, eval=F, include=F}
###             ###
### not working ###
###             ###  

modeling_data

dtrain <- 
  xgb.DMatrix(
    data = as.matrix(modeling_data %>% select(-species_Aglais_caschmirensis)), 
    label = as.matrix(modeling_data %>% select(species_Aglais_caschmirensis))
    )


xgb_model <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 200, objective = "binary:logistic", verbose = T)

xgb.plot.shap( data = as.matrix(modeling_data %>% select(-species_Aglais_caschmirensis)), model = xgb_model, top_n=7)

#xgboost_pred <- predict(model, as.matrix(species_absence))



```

## Model results

Interessting species predictions:
- Tarucus_theophrastus
- Lethe_verma
- Junonia_iphita


predictions were made for points outside pakistan. There should be no data extracted for those points. How ist this possible?
Cut samples with polygon of pakistan!

Species with small number of samples resuting in no prediction. Where ist the min the predicton works? Check Species with 1, 2, 3, 4, 5 Samples and so on 






Species Richness Map


# Conclusion

How was Computing Speed compared to other methods? 


print sessioninfo