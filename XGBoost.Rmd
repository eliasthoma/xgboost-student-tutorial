---
title: "Tutorial: Using XGBoost for Species-Distribution-Modelling"
output: 
  html_document:
    theme: united
    numbered: TRUE
    number_section: TRUE
    toc: TRUE
    toc_float: TRUE
params:
    dev: TRUE # shortens knit time, by only modeling one species
    output_plots: FALSE
    gpu_acc: FALSE
---

<style type="text/css">
 { /* Normal  */
      
  }
 body .main-container {
        max-width: 2000px;
        font-size: 16px;
    }
td {  /* Table  */
  font-size: 9px;
}
h1.title {
  font-size: 20px;
  color: DarkRed;
}
h1 { /* Header 1 */
  font-size: 18px;
  color: DarkRed;
}
h2 { /* Header 2 */
    font-size: 16px;
  color: DarkRed;
}
h3 { /* Header 3 */
  font-size: 14px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 14px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 12px;
}
</style>

<!-- style for image slider  -->
<style>
  * {box-sizing:border-box}

/* Slideshow container */
.slideshow-container {
  max-width: 800px;
  position: relative;
  margin: auto;
}

/* Hide the images by default */
.mySlides {
  display: none;
  padding: 0px 30px;
}

/* Next & previous buttons */
.prev, .next {
  cursor: pointer;
  position: absolute;
  top: 50%;
  width: auto;
  margin-top: -22px;
  padding: 16px;
  color: white;
  font-weight: bold;
  font-size: 18px;
  transition: 0.6s ease;
  border-radius: 0 3px 3px 0;
  user-select: none;
  background-color: rgba(0,0,0,0.8);
}

/* Position the "next button" to the right */
.next {
  right: 0;
  border-radius: 3px 0 0 3px; 
}

/* On hover, add a black background color with a little bit see-through */
.prev:hover, .next:hover {
  background-color: rgba(0,0,0,0.8);
}

/* Caption text */
.text_old {
  color: #f2f2f2;
  font-size: 15px;
  padding: 8px 12px;
  position: absolute;
  bottom: 8px;
  width: 100%;
  text-align: center;
}

.text {
  color: black;
  font-size: 15px;
  padding: 8px 12px;
  width: 100%;
  text-align: center;
  min-height: 60px;
}

/* The dots/bullets/indicators */
.dot {
  cursor: pointer;
  height: 15px;
  width: 15px;
  margin: 0 2px;
  background-color: #bbb;
  border-radius: 50%;
  display: inline-block;
  transition: background-color 0.6s ease;
}

.active, .dot:hover {
  background-color: #717171;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "hold")
```
# ) Dummy notes area

https://juliasilge.com/blog/baseball-racing/
XGBoost-Docs: https://xgboost.readthedocs.io/en/stable/
https://dieghernan.github.io/tidyterra/reference/geom_spatraster.html
https://www.youtube.com/watch?v=gKyUucJwD8U&list=WL&index=71
https://www.geeksforgeeks.org/xgboost/

cbind: besser merge benutzen

Interaktive Table:
datatable(modeling_data)



Senay, S. D., Worner, S. P., & Ikeda, T. (2013). Novel three-step pseudo-absence selection technique for improved species distribution modelling. PloS one, 8(8), e71218. Online availible under https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3742778/pdf/pone.0071218.pdf. Last checked 02.06.2022.

# ) Introduction

The following Tutorial is the final assessment of the project-seminar: “Species Distribution Modeling” at Philipps-University Marburg. In this tutorial we're going to use the XGBoost algorithm to predict the specie´s distribution of butterflies in Pakistan and create a species richness map of the country. [XGBoost](https://cran.r-project.org/web/packages/xgboost/xgboost.pdf) (eXtreme Gradient Boosting) is a popular machine learning algorithm that belongs to the family of gradient boosting methods. It was developed by [Tianqi Chen](https://tqchen.com/). and uses a combination of gradient boosting, decision trees, regularization, gradient-based optimization, feature importance analysis and parallelization. All this make's it a robust and powerful algorithm that often delivers state-of-the-art results in various machine learning tasks.
You will be introduced to the basic concepts of XGBoost and we'll provide a reproducible workflow to use XGBoost to build classification models.


# ) So how does XGBoost work?

XGBoost is a ensemble Method such as Random Forrest, this means it combines the output of multiple Trees. But the methods differ in the way the idividual Trees are build and how the results are combined.
In Xgboost the output oft the trees aren't combined equally. Instead XGBoost uses a method called boosting.
Boosting combines weak learner (small trees) sequentually so that the new tree corrects the errors of the previous one.
To understand this we have to look into some mathematical details. But dont worry when using XGBoost these details will be automated.
Nevertheless its importand to understand these processes to optimize the algorithm later on.

As said XGBoost builds on many concepts to deliver it's results.
We're going to start with how XGBoost builds trees and then progress trough the different tuning-parameters.


<!-- Slideshow container -->
<div class="slideshow-container">

<div class="mySlides">
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhrAdT9U5a6js_clc19U2MySmViR5NAdMz5acOyTkolprWMt5-xYqLO8LgRgnCtHyt3UdKScfZtVNuoSkWNu8DfJe8aOeH4cxn1uWd7AarUOSMMOT5IPmMYegkyuxvN6qSBp26iFepdLY4FvCGs5XW9aO8rjT7tpsVNaotlPT0TcCWalyMXhddxZHwLxng/s1600/image1.jpeg" style="width:100%">  
<div class="text">In this assessment were trying to classify geo-points if they're potential habitats for a species or not. In order to do that we need XGBoost to build binary-regression trees, thus classifying a given point if a condition is met (=1) or not (=0). In our case the green dots, with a value of 1, represent the presence points. Red dots, with values of 0 represent absence points. The black line in the middle is XGBoost's initial prediction. By default, it is 0.5, which means there is a 50% chance to find a butterfly at any given point.</div>
</div>

<div class="mySlides">
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDNS7NaU14obTeIdPoNMZGf3zl6y5PtL6RFqW4R9DpyQnJ08VoXx656ubQ4YYmn7SkdDI8TSyfnZtZeLmZ0rdd1gdsOn1vnVsmow7wsfptuMCMm7cxQyxHs6JVcdqhE7pMaJ6e6mP7abHtEkrnOelaK0oIBGpnWhpoEnldhUgK3EFyBqT_8JAyAYIDHg4/s1600/image2.jpeg" style="width:100%">
<div class="text">In the next step XGBoost calculates the residuals of all given points. The residuals are the difference between the observed prediction and the predictet. If the observed value is one (i.e., a presence point), the residuals are 0.5. The same applies to values of zero (i.e., absence points) where the residuals are -0.5.</div>
</div>

<div class="mySlides">
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgRsgLPZq5EfK0nt-jMyi2dRi3dg0CXvEGYVaKdTCZuKMYVymUEGzV3dTlGZ8biZnw2nZuU64y6bQnEz2MzByxq_oc_KMakRqAqpSuAY8yNvqLS-eGfpLXLRGsFzeDMpGH20jEoejFqcOoL56z3uWfHI-h2-tLvnEPZgCxdjn8VQfCJGsCH7lVY3W1qWWg/s1600/image3.jpeg" style="width:100%">
<div class="text">In order to find the threshhold of parameters that influence the probability of a point being a presence or absence point the data has to be split at the crucial values of that parameter.</div>
</div>

<div class="mySlides">
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhM9-yn-gtedu2-iH4cr4yfeC4zVSV-vaXaPCNIL2ZTi1-UdAhdBtopQDIlPkeWj85zUpy5yVD9vMMn-M3wyTMUPcDplwORV6qGLA4SkLhPGsUlzy-YOOvyBUV40M330ReHUC-hVXpn5f0aZeCc2Fyeogfe51Ci8DIMI2qxM1LTqnsbVTWn2pFTIx0nqM/s1600/image4.jpeg" style="width:100%">
<div class="text">Therefore XGBoost splits the observations at thresholds that result in the highest gain value. But whats the gain-value?</div>
</div>

<div class="mySlides">
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg7R0oWdDQqkK6MXSD6zqchi27Fsun5UK2aal9TcArMU_jUCPJEh4VbxCa--ZxVHijWvU_y3vPmk87C9ToUjK856o3DpQCGDSG5fPkcs8GGRIfs3iUmtSgFzS2FVum_dFcvG5w9_OlfLd9DrCXI9Eh8MlSIQ4YIcF0_w6FwBHh9SCjIqq24xbrlV-w0Irg/s1600/image5.jpeg" style="width:100%">
<div class="text">To calculate the gain-value we need the similarity-score of each leaf and the root.</div>
</div>

<div class="mySlides">
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhelYfzAkKKurpxMj1xCuwF0aLNWrJ2JSSqs5OFXNyxFNKfr8GXcNgDLHqAg8KnNfI3hDsviapqnKO5elMpnHAZLYGZNL-wEzMEL7LdT55k3F9aHajhAtxQc1JkcQNdeYh4601gcsCDbU1wmaokkadKOBPd347OEJHQFjNQcQ_qQV_cM2kXOZ3l3Z0VrCg/s1600/image6.jpeg" style="width:100%">
<div class="text">By summing the similarity-scores of the left and the right leaf and then substracting the similarity-score of the root we get the gain-value. In this case it's 3.8 and the highest possible for this dataset therefore this would be the final tree. But why doesn't XGBoost split the residuals any further? This has to do with regularization parameters and pruning wich we're going to explain in the next chapter.</div>
</div>


  <!-- Next and previous buttons -->
  <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
  <a class="next" onclick="plusSlides(1)">&#10095;</a>
</div>

<!-- The dots/circles -->
<div style="text-align:center">
  <span class="dot" onclick="currentSlide(1)"></span>
  <span class="dot" onclick="currentSlide(2)"></span>
  <span class="dot" onclick="currentSlide(3)"></span>
  <span class="dot" onclick="currentSlide(4)"></span>
  <span class="dot" onclick="currentSlide(5)"></span>
  <span class="dot" onclick="currentSlide(6)"></span>
</div>

<script>
let slideIndex = 1;
showSlides(slideIndex);

// Next/previous controls
function plusSlides(n) {
  showSlides(slideIndex += n);
}

// Thumbnail image controls
function currentSlide(n) {
  showSlides(slideIndex = n);
}

function showSlides(n) {
  let i;
  let slides = document.getElementsByClassName("mySlides");
  console.log(slides)
  let dots = document.getElementsByClassName("dot");
  console.log(dots)
  if (n > slides.length) {slideIndex = 1}
  if (n < 1) {slideIndex = slides.length}
  for (i = 0; i < slides.length; i++) {
    slides[i].style.display = "none";
  }
  for (i = 0; i < dots.length; i++) {
    dots[i].className = dots[i].className.replace(" active", "");
  }
  slides[slideIndex-1].style.display = "block";
  dots[slideIndex-1].className += " active";
}
</script>

# ) Regularization & Pruning

How XGBoost builds trees is limited by  multiple regularization parameters:

## ) Lambda

We've heard of Lambda when we're calculated the similarity-score. XGBoost default value for Lambda is 0 therefore we've been ignoring it. But when Lambda is set to > 0 the similarity-score get's smaller because the denominator becomes larger. Thus Lambda prevents over-fitting.

</center>

```{r, echo=FALSE, out.width = '30%'}

knitr::include_graphics("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhmPwBMb_KTLxryPlbQUksG13KoiKE97D-lH2vseNhSeeHv4M8Vrz8yTG8lMvyF6EIWt5LFwxCUN9Fb1JqZwjJi0H_kiX5agdqr2_KSqKkuEN3C6ed0wAqqjp7-2nl310WWnxgfCObLfHxPWjNNyT01BIcK_GQcFjWlD4xA3CRZ2kL_iCRaQMU_KXC7zx8/s1600/lambda.jpg")
```

## ) Cover or min_child_weigth

Another regularization parameter is the cover or min_child_weight. This parameter is also the reason why we haven't continued building our example tree. In XGBoost the default value for the cover is 1 wich means that every leaf with a cover-value less than 1 get's pruned. When building regression-tree's
the cover or min_child_weight of a leaf is just the number of residuals in the leaf. Wereas the cover for for binary-regression tree's is calculated by summing the previous probability times 1 minus the previous probability, for each residual in the leaf.

```{r, echo=FALSE, out.width = '100%'}

knitr::include_graphics("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgm8a-UAaRkFn0z90jWb4YuwIX6ExK1dE2yC_vOMmYls_BxTAdHKoHYfazNgYZwxzLCLhc3XMbyzOE63O3Azt8iEIcvBSxiLQfz40AkeL_OSDSeqY9BKrYAIg9IHG9hmANyurL1UxwmMSCMXNA5MS1fuG_Fzdf5_ynlX4bM4T8zdADbqh-nqZ1Vyk9QwpY/s1600/image7.jpg")
```

## ) Gamma

Similar to cover or min_child_weigth gamma is a regularization parameter that causes XGBoost to prune leafs.
Gamma is a highly dependent regularization parameter, what mean's that there is no "good"-default-value. By default it's 0, therefore no regularization takes place. If a gamma-value > 0 is used XGBoost substract's gamma from the gain-value of each leaf and then removes all the leaf's with a negative result.

```{r, echo=FALSE, out.width = '100%'}

knitr::include_graphics("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaz4IyygAkr4KxI2BiVDzw2GjUT25tokUIU1BkwoSLWOz6A7Km_CnAImcm7cY_7gBPi3juIUEkYCNW2MI5NVnSPjqMxYp0C-mG1MlH-eI-BWwZXEK7sNhW9Ki87HYu0DZHIGTGafT6J7f7msh9uh7BjTMyX4rT9nKDsIzjpX5kv-Pusaz7nhkJAClafnE/s1600/gamma.jpg")
```

For example if we take the previous calculated gain-value of our example tree of 3.8, a gamma-value of 4 would prune the whole tree down to the root. Thus no prediction. In contrast to this with the default-gamma-value of 0 XGBoost build's extremly large trees, thus overfitting the trees to the dataset and raising the computation time a lot.
Therefore gamma prevents overfitting the trees to our data and make's the prediction more conservative but can also slow the whole process by preventing tree's being build.

# ) New prediction.

When the first tree is build the output-value of each leaf is calculated by dividing the sum of the residuals of a leaf and dividing the result by the previous prediction times 1 minus the previous prediction (for each residual) plus lambda.

```{r, echo=FALSE, out.width = '100%'}

knitr::include_graphics("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjUcdme4IZU671hpqlujcljwbQE-oWVBHflZs3vzQlp-FAi9L46291m-IF-mNyWWCvO2YWuVFRXULWYZ5YfvfsW48_G-i5ejVmEDUhJ79RHVPRVLI3JT3KzZh-eI3rVM2r-t_VxDj2fKzUXgLlpXr_YkkcQYK63N87Xn1VvCImshOJwzM19P5fIts3ItFA/s1600/output.jpg")
```

For the leaf containing the residuals of our presence-point's it would look like this:

```{r, echo=FALSE, out.width= "100%"}
knitr::include_graphics("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimErCByaFe-qpUj1RPW7ijsR4GrGhKfX0AMU7BIwphLTju88SHPYXbtjF1LymqbeW7TElDVVwroGnMPVl2m8H6OZ9Hw2k1Efu2UMjo_wpU6565bayGjmc9Xt6pHhSx7Qvv0oXpNNnWcM3Zvqcfvy5gICuB6xUANorEFm_zNOXEYH_4HXq0tbqljTSaCBM/s1600/output_leaf.jpg")
```
After calculating the output-value the new prediction for the presence-point's is calculated by adding the log(odd's) to the learning-rate and multiplying with the output-value and then converting it to a logistic-function-probability.

```{r, echo=FALSE, out.width = '100%'}

knitr::include_graphics("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj49rPF3O8kucjs5gN06REIWy1I8oH8wuY4JYGHa2dyeA2F-0nz7_hgbSqD3IXZSZVaZx82g3xAlfINBRbyIdz0TbEAOxXz9sxQZXYIIUR0JnK7_E8_-gUMsf69tmsUJUjlK4HFFmR_HyMt4BiIfQgKXdrJrHwbCduGeakIu5mDiXlURMi6jkamjM64Ogk/s1600/new_prediction_math.jpeg")
```

Therefore the initial prediction for our presence-points in the next tree would be 0.64.
```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdBh6P6xgI7Y13NUK_WWV-XL1X5Re7AbLAzyhAL2I8hZBMAIxj8WQsVvhF9p80aTI6yLcvvvoS9V_4q4Daa5CODP4qn2prjl5ocH32Admdm30N3yS8erY1YdrfEDs-APTpBn0jjtjrWv5N3B-hoeFFLSv2gzTfNl5S8vXoi6D8hCSgzAPZtbG0rkko0eY/s1600/new_prediction_graph.jpg")
```

# ) Greedy Algorithm & Weighted quantiles



# ) Application of XGBoost in R

XGBoost is available for different programming and scripting languages, include Python, R, Java and C++. 
Docuementation is available here: https://xgboost.readthedocs.io/en/stable/

## ) Prerequisites

Before you dive into the code you need to install some packages this script will use: 

```{r Install packages, eval=FALSE}
install.packages("terra")
install.packages("ggplot2")
install.packages("fastDummies")
install.packages("tidyterra")
```

The XGBoost package can by installed in two different ways. First there is the default package from CRAN, which will do it in most situations.
```{r Setup from CRAN, eval=F}
#install.packages("xgboost")
```
But if you are dealing with large data sets you may want to use GPU acceleration. Therefor you have to use a prebuild package from GITHUB (https://github.com/dmlc/xgboost/releases). Download it, place it in the same folder as this script and run the commands below. For Benchmarks of a High-End CPU vs Low-End GPU, see https://medium.com/data-design/xgboost-gpu-performance-on-low-end-gpu-vs-high-end-cpu-a7bc5fcd425b. Risks? Reproducability?
```{r Setup for GPU acceleration, eval=FALSE}
##
## !! Installation on windows failed with "Warning in system("sh ./configure.win") 'sh' not found", for dirty Solution see section Troubleshooting
##

detach("package:xgboost")

# Install dependencies
system('R -q -e "install.packages(c(\'data.table\', \'jsonlite\'))"') # TODO: convert this to r code
# Install XGBoost
system(paste("R CMD INSTALL ", getwd(),  "/xgboost_r_gpu_win64_21d95f3d8f23873a76f8afaad0fee5fa3e00eafe.tar.gz", sep=""))

require(xgboost)

params$gpu_acc = TRUE
```

After installing all needed libaries you need to load them: 

```{r Loading libaries, results='hide', message=FALSE}
# TODO: use this line:
# if(!require(dplyr)){install.packages('dplyr')}

require(dplyr)      # easy dataframe modification
require(ggplot2)    # plotting

require(geodata)    # downloading geospatial world dataset made easy
require(sf)         # simple geospatial features
require(terra)

require(tidyterra)  # plot terra object with ggplot

require(fastDummies)# create binary factor columns from character column
require(xgboost)    # our modeling libary
```

## ) Data preparation

Let's begin with preparing the data used to train the model. Start with getting a overview of the provided data:

```{r Loading species data}
species_occurrences_all <- read.table("data/PakistanLadakh.csv", sep=",", header=TRUE)
species_occurrences_all <- sf::st_as_sf(species_occurrences_all, coords=c("x", "y"), remove=TRUE, crs=sf::st_crs("epsg:4326"))

str(species_occurrences_all)
```
```{r, include=FALSE}
if(params$subset_samples)
{
  species_names <- (species_occurrences_all %>% distinct(species))$species
  species_occurrences_all = species_occurrences_all %>% filter(species %in% c("Aglais_caschmirensis", species_names[1]))
  rm(species_names)
}
```

```{r}
ggplot() + 
  geom_sf(data = sf::st_as_sf(species_occurrences_all), mapping=aes(color=species), show.legend = FALSE) +
  ggtitle("Oberserved occurrence of butterflies in Pakistan")
```

Next we need some environmental data to train the model. Therefor we selected the bioclim data, which are widely used in speceis distribution modeling (Source: https://isprs-archives.copernicus.org/articles/XLII-4-W19/449/2019/isprs-archives-XLII-4-W19-449-2019.pdf#:~:text=The%20earliest%20studies%20of%20SDM%20used%20BIOCLIM%20-,requires%20species%20occurrence%20data%20%28latitude%2C%20longitude%2C%20and%20elevation%29.). Bioclim is missing elevation data, we will use those too, since temperature is depend on elevation. The Border of Pakistan is also needed, we will crop our data with that, so the model doesn't train areas were does not have presence points.

```{r Loading environmental data, message=FALSE, warning=FALSE}
# political border of pakistan
border_pak <- geodata::gadm(country='PAK', level = 0, path='./data')
ggplot() +   
  geom_sf(data = sf::st_as_sf(border_pak), fill=NA)

# bioclim data from pakistan
bioclim_pak <- geodata::worldclim_country(country = "PAK", res = 10, var = "bio", path = "data/", version = "2.1")
names(bioclim_pak) <- substr(names(bioclim_pak), 11, 20) # TODO: rename to mare meaningful names or show table of layers in text
bioclim_pak <- terra::mask(bioclim_pak, border_pak)
ggplot() + 
  geom_spatraster(data = bioclim_pak) +
  facet_wrap(~lyr) +
  geom_sf(data = sf::st_as_sf(border_pak), fill = NA, show.legend = FALSE) + 
  ggtitle("Bioclim data of Pakistan")

# elevation data form pakistan
elevation_pak <- geodata::elevation_30s(country = 'PAK', path = 'data/')
ggplot() + 
  geom_spatraster(data = elevation_pak) +
  geom_sf(data = sf::st_as_sf(border_pak), fill = NA, show.legend = FALSE) +
  scale_fill_hypso_c(name = "Elevation")
```

So lets define our species_occurrences as presence points : 

```{r presence points}
species_presence <- species_occurrences_all
rm(species_occurrences_all)
```

As absence points we will user random Points in Pakistan and combine them with the presence points. The absence points will be extended by a column "species", which matches the column "species"´in the presence points.

```{r absence points, warning=FALSE}
# Generate random points inside pakistan as background points and extend them with a column for species = NA
# TODO: why 1000 points?? give a explanation for the decision
border_pak <- sf::st_as_sf(border_pak)
species_absence <- sf::st_sample(border_pak, size = 1000)

# TODO: add col for occurrence = 1 or occurrence = 0 here, dummycols can then be thrown away
# adding col species = NA to the background points, needed for rbind to join the data
species_absence <- cbind(species_absence, data.frame(species = as.character(NA)))
species_absence <- sf::st_as_sf(species_absence)

# Combine presence and absence (background) points into a single object
modeling_data_ <- rbind(species_presence, species_absence)

# Only points inside Pakistan should be used for modeling, also remove the columns added by the intersection. 
modeling_data_ <- sf::st_intersection(modeling_data_, border_pak) %>% select(-COUNTRY, -GID_0)
```

Now we got our presence and absence points as spatial data. Finally we will extract values from our environmental data and add those to our modeling data, so xgboost can use this table to train its model.

```{r extract}
# Extract values from bioclim and elevation, join them to our modeling_data
extraction_bioclim_pak <- terra::extract(bioclim_pak, modeling_data_, bind=FALSE, ID=FALSE)
extraction_elevation_pak <- terra::extract(elevation_pak, modeling_data_, bind=FALSE, ID=FALSE)
modeling_data_extracted <- cbind( modeling_data_, extraction_bioclim_pak, extraction_elevation_pak)
```

Clean up of no longer needed variables and check the final modeling data:

```{r clean up}
# create a final data variable and clean up variables
modeling_data <- modeling_data_extracted

rm(species_occurrences_all); rm(species_presence); rm(species_absence); rm(modeling_data_); rm(extraction_bioclim_pak); rm(extraction_elevation_pak); rm(modeling_data_extracted)

str(modeling_data)

ggplot() + 
  geom_sf(data = sf::st_as_sf(border_pak), fill=NA, show.legend=FALSE) +
  geom_sf(data = sf::st_as_sf(modeling_data), mapping=aes(color=species), show.legend = FALSE) +
  ggtitle("Oberserved occurrence of butterflies in Pakistan plus background points")
```

```{r Plot - Number of Samples per species}
species_nsamples = data.frame(modeling_data) %>% 
                    count(species, sort=TRUE) %>% 
                    rename(n_samples = n) %>% 
                    filter(!is.na(species))

ggplot(species_nsamples, aes(n_samples)) +
       geom_histogram(binwidth = 5) +
       geom_vline(aes(xintercept=mean(n_samples)), linetype="dashed") +
       annotate(x=mean(species_nsamples$n_samples), y=+Inf, label=paste("Mean:",round(mean(species_nsamples$n_samples),2)), vjust=3, geom="label") +
       labs(x = "Number of Samples", y = "Number of Species")

rm(species_nsamples)
```

## ) Training  and prediction with xgboost

Starting with the training of our xgboost model we decided to do sperate training for every model. Despite that xgboost is capable of Multi-Classificiation. Therefor we defined a function 'train' wich invokes out data filtering and xgboost specific data preperation to meet the requirements of xgboost. Espacially converting the modeling data into a 'xgb.DMatrix' object. Finally we define some general parameters we want to use, e.g. enable of gpu acceleration by knit paramters. Last but not least we save the model to the disk to preserve it for modeling.


```{r training function}
train <- function(
    data, # training data, required
    sp, # species name, required
    xgb_params = list("nrounds" = 1000), # xgboost params, see https://xgboost.readthedocs.io/en/latest/parameter.html
    save_model = TRUE 
) {
# filter modeling data to current species, don't forget the absence points!
data <- modeling_data %>% filter(species == sp | is.na(species))
data <- dummy_columns(data, select_columns = "species",
                      ignore_na = TRUE, 
                      remove_selected_columns = TRUE)
  
# make dummy_columns name persistent over all species
# replace NA values with 0
data <- data %>% 
  rename(species_occurrence = starts_with("species_")) %>%
  mutate(species_occurrence = ifelse(is.na(species_occurrence), 0, species_occurrence)) %>%
  mutate(species_occurrence = factor(species_occurrence))
  
# xgboost need a specific data format  
data <- xgb.DMatrix(
  data = as.matrix(data %>% select(-species_occurrence, -geometry)), 
  label = as.matrix(data %>% select(species_occurrence))  
)

# enable gpu acceleration only if wanted
if(params$gpu_acc) {
  xgb_params = c(xgb_params, predictor="gpu_predictor")
  xgb_params = c(xgb_params, tree_method="gpu_hist")
}
#xgb_params = c(xgb_params, eval_metric="error") # Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.
xgb_params = c(xgb_params, objective = "binary:logistic") # logistic regression for binary classification, output probability
xgb_params = c(xgb_params, sampling_method = "gradient_based")

model <- xgboost(data = data,
                 verbose = 0,            # 0 (silent), 1 (warning), 2 (info), 3 (debug)
                 nrounds = xgb_params$nrounds,
                 params = xgb_params
                 )

message(paste("train_logloss:", mean(model[["evaluation_log"]][["train_logloss"]])))
  
if(save_model)
{
  xgb.save(model, paste("out/", sp, ".model", sep = ""))  # Max compatibility with future xgboost versions
  save(model, file = paste("out/", sp, ".rds", sep = "")) # Fully restorable r object
}
  
return(model)
}
```

Additional training paramters are defined here in a own data frame. Advantage of this are the comparability and useability of the different parameter sets. The Table show the default parameter, one set taken from Roozbeh Valavi et. al. And last the parameters we will user for training. Those have been tested and tuned manually.


```{r training parameter}
rm(xgboost_params)

xgboost_params <- data.frame("dataset" = character(), 
                             "nrounds" = numeric(), 
                             "eta" = numeric(), 
                             "max_depth" = numeric(), 
                             "subsample" = numeric(), 
                             "gamma" = numeric(),
                             "alpha" = numeric(),
                             "lambda" = numeric(),
                             "colsample_bytree" = numeric(), 
                             "min_child_weight" = numeric()
                             )

xgboost_params <- xgboost_params %>% add_row(dataset = "default", eta = 0.3, max_depth = 6, gamma = 0, alpha = 0, lambda = 1, subsample = 1, colsample_bytree = 1, min_child_weight = 1)

# Parameter taken from https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/ecm.1486
xgboost_params <- xgboost_params %>% add_row(dataset = "literature", nrounds = 1000, eta = 0.001, max_depth = 5, subsample = 0.75, gamma = 0, alpha = 0, lambda = 1, colsample_bytree = 0.8, min_child_weight = 1)

xgboost_params <- xgboost_params %>% add_row(dataset = "tuned 1", nrounds = 2000, eta = 0.01, max_depth = 5, subsample = 1, gamma = 0, alpha = 0, lambda = 1, colsample_bytree = 1, min_child_weight = 1)

xgboost_params <- xgboost_params %>% add_row(dataset = "tuned 2", nrounds = 5000, eta = 0.3, max_depth = 5, subsample = 1, gamma = 2, alpha = 0, lambda = 1, colsample_bytree = 1, min_child_weight = 1)

xgboost_params <- xgboost_params %>% add_row(dataset = "tuned 3", nrounds = 3000, eta = 0.15, max_depth = 7, subsample = 0.75, gamma = 1.5, alpha = 0, lambda = 0.5, colsample_bytree = 1, min_child_weight = 1)

print(xgboost_params)
```

Next is to perpare data used to predic species occurrence in pakistan. Therefore we will use the raw raster data and predict of those with 'terra::predict' which allows us to pass on a 'Spatraster' object. XGBoost can't handle Spatraster, so 'terra:predict' allows as to define a custom prediction function, which converts data into a matrix.

```{r prediction data}
# gen stack from rasters bioclim_pak and elevation_pak
elev_pak = resample(elevation_pak, bioclim_pak)
ext(elevation_pak) <- ext(bioclim_pak)
prediction_rstack = c(bioclim_pak, elev_pak)

# Remove values outside pakistan, because otherwise the model will make predictions outside the modeling area
prediction_rstack = terra::mask(prediction_rstack, border_pak)
```


```{r prediction function}
# We need to make a custom predict function for terra::predict() since xgboost didn't take a data.frame as input. See https://stackoverflow.com/questions/71947124/predict-xgboost-model-onto-raster-stack-yields-error
prediction_custom <- function(model, data, ...) {
  stats::predict(model, newdata=as.matrix(data), ...)
}

predict <- function(model, prediction_data, plots=FALSE)
{

  #model = xgb.load(paste("out/", sp, "/" ,sp, ".model", sep = ""))
  #model = readRDS(paste("out/", sp, "/" ,sp, ".bin", sep = ""))
  prediction = terra::predict(object=prediction_data,
                       model=model,
                       fun=prediction_custom
  )
  
  terra::writeRaster(prediction, paste("out/", sp, ".tif", sep = ""), overwrite=TRUE)
  return(prediction)
}
```

## ) Example model for species "Aglais_caschmirensis"

Three different paramter sets used for Aglais_caschmirensis

```{r Sample Aglais_caschmirensis 1}
sp = "Aglais_caschmirensis"

print(xgboost_params[2,])


model <- train(modeling_data, sp, as.list(xgboost_params[2,]), save_model = FALSE)
prediction <- predict(model, prediction_rstack)
ggplot() + 
  geom_spatraster(data = prediction) +
  scale_fill_hypso_c(direction = -1,
                     limits=c(0,1),
                     name = "Prediction") +
  geom_sf(data = modeling_data %>% filter(species == sp),
          size = 1,
          shape = 1 ) +
  geom_sf(data = sf::st_as_sf(border_pak),
          fill = NA, show.legend=FALSE) +
   ggtitle(paste("Oberserved and predicted occurrence of", sp, "in pakistan \n", paste("train_logloss:", mean(model[["evaluation_log"]][["train_logloss"]])), "\n params_index: 2"))

rm(sp)
```

```{r Sample Aglais_caschmirensis 2}
sp = "Aglais_caschmirensis"

print(xgboost_params[4,])

model <- train(modeling_data, sp, as.list(xgboost_params[4,]), save_model = FALSE)
prediction <- predict(model, prediction_rstack)
ggplot() + 
  geom_spatraster(data = prediction) +
  scale_fill_hypso_c(direction = -1,
                     limits=c(0,1),
                     name = "Prediction") +
  geom_sf(data = modeling_data %>% filter(species == sp),
          size = 1,
          shape = 1 ) +
  geom_sf(data = sf::st_as_sf(border_pak),
          fill = NA, show.legend=FALSE) +
   ggtitle(paste("Oberserved and predicted occurrence of", sp, "in pakistan \n", paste("train_logloss:", mean(model[["evaluation_log"]][["train_logloss"]])), "\n params_index: 4"))

rm(sp)
```

```{r Sample Aglais_caschmirensis 3}
sp = "Aglais_caschmirensis"

print(xgboost_params[5,])

model <- train(modeling_data, sp, as.list(xgboost_params[5,]), save_model = FALSE)
prediction <- predict(model, prediction_rstack)
ggplot() + 
  geom_spatraster(data = prediction) +
  scale_fill_hypso_c(direction = -1,
                     limits=c(0,1),
                     name = "Prediction") +
  geom_sf(data = modeling_data %>% filter(species == sp),
          size = 1,
          shape = 1 ) +
  geom_sf(data = sf::st_as_sf(border_pak),
          fill = NA, show.legend=FALSE) +
   ggtitle(paste("Oberserved and predicted occurrence of", sp, "in pakistan \n", paste("train_logloss:", mean(model[["evaluation_log"]][["train_logloss"]])), "\n params_index: 5"))

rm(sp)
```


```{r, inculde = FALSE, eval=FALSE}
  if(plot) {
    #xgb_model[["evaluation_log"]]
    
    importance <- xgb.importance(model = model)
    #xgb.plot.importance(importance_matrix = importance)
    
    #xgb.ggplot.deepness(xgb_model)
    
    #xgb.plot.multi.trees(mode = xgb_model, features_keep = 3)
    
    #library("DiagrammeRsvg", "rsvg")
    
    
    #gr <- xgb.plot.multi.trees(model=xgb_model, features_keep = 5, render=FALSE)
    #DiagrammeR::export_graph(gr, 'tree.pdf', width=600, height=1500)
    
    xgb.ggplot.shap.summary(data = as.matrix(modeling_data %>% select(-species_occurrence, -geometry)), model = model )
    ggsave(paste(sp, "_shap.png", sep=""), path=paste("out/",sp, sep=""))
    rm(importance)
  }
```


## ) Species Richness Map

Finally we combine all predicted species occurrence into a Map that indicates how many species might occur in one pixel. First, we define a threshold above which the prediction should be considered. The prediction have been saved as tif in 'out/<species_name>.tif' and we need to load them before modifying. After that, we can reclassify the raster with 0 and 1, based on the threshold. To get the number of species in one pixel we need to sum up all rasters into one final raster and plot it using ggplot.

```{r loop, message=TRUE, warning=FALSE, error=FALSE}

time_start <- proc.time()
i <- 0

species = (modeling_data %>% distinct(species) %>% filter(!is.na(species)))$species

# xgboost needs the output dir to exist before saving model
system("mk out")

for(sp in species)
{
  time_start_species <- proc.time()
  i <- i+1
  message(paste("[", i, "/", length(species), "] ", sp, ": ", sep = ""), appendLF=F)
  
  # training and prediction the species model
  model <- train(modeling_data, sp, as.list(xgboost_params[5,]))
  prediction <- predict(model, prediction_rstack)
  
  
  ggplot() + 
    geom_spatraster(data = prediction) +
    scale_fill_hypso_c(direction = -1,
                       limits=c(0,1),
                       name = "Prediction") +
    geom_sf(data = modeling_data %>% filter(species == sp),
            size = 1,
            shape = 1 ) +
    geom_sf(data = sf::st_as_sf(border_pak),
            fill = NA, show.legend=FALSE) +
    ggtitle(paste("Oberserved and predicted occurrence of", sp, "in pakistan \n", paste("train_logloss:", mean(model[["evaluation_log"]][["train_logloss"]]))))
  ggsave(paste(sp, ".png", sep=""), path="out")

  message(paste("Exceution took", proc.time() - time_start_species, "seconds"))
}

message(paste("Total Exceution took", proc.time() - time_start, "seconds"))
```

```{r species richness map}
# Step 1  generate Raster Stack
# l_species will consist of all 421 species prediction rasters -> RAM usage will be insane ~ 25GB
l_species = list()

threshold = 0.5

# reclassify raster:
# value < threshold = 0
# value > threshold = 1
m <- c(0, threshold, 0,
       threshold, 1, 1)
m <- matrix(m, ncol=3, byrow=TRUE)

for(sp in (modeling_data %>% distinct(species) %>% filter(!is.na(species)))$species)
{
  # get species raster from file system
  r <- rast(paste("./out/",sp,".tif", sep = "" ))

  l_species[sp] <- terra::classify(r, m, include.lowest = TRUE)

  rm(r)
} 

stack = terra::rast(l_species)
stack = sum(stack)
stack = terra::mask(stack, border_pak)

# TODO: mask stack to pakistan

ggplot() + 
  geom_spatraster(data = stack) +
  scale_fill_hypso_c(palette="spain", name="N° of species" ) +
  #scale_fill_hypso_b(name="N° of species") +
  geom_sf(data = sf::st_as_sf(border_pak), fill=NA, show.legend=FALSE) +
  ggtitle("Species richness of butterflies in Pakistan")

ggsave("SpeciesRichnessMap.png", path="out") 

```


For how many species is no prediction made?
-> count(max(raster) = 0)

# ) Conclusion

# ) Sources

# ) Troubleshooting

**Installing XGBoost with GPU accerlation**

Pre-built binary packages are offered by XGBoost after someone makes a request on GitHub (https://github.com/dmlc/xgboost/issues/6654). Since the packages are precompiled, it should not be necessary to compile them from source. The installation still fails with error `r `knitr::inline_expr("Warning in system("sh ./configure.win") 'sh' not found")`. So there are two ways to fix this problem:
- Install R-Tools and build from source
- Copy src/xgboost.dll from archive (https://github.com/dmlc/xgboost/releases) into your r library manually e.g C:\Users\%USERNAME%\AppData\Local\R\win-library\4.2\xgboost\libs\x64

**Sessioninfo**

```{r sessioninfo}
sessionInfo()
```

